{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d10fa48-8bab-42cb-9656-82d25c732599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da7ec6dd-790a-4a16-8c9d-88f074c15300",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/bandar_abbas_1_year.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m DATA_NAME = \u001b[33m\"\u001b[39m\u001b[33mbandar_abbas_1_year.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDATA_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sklearn-env/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sklearn-env/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sklearn-env/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sklearn-env/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/sklearn-env/lib/python3.13/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../../data/bandar_abbas_1_year.csv'"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"../../data/\"\n",
    "DATA_NAME = \"bandar_abbas_1_year.csv\"\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(os.path.join(DATA_DIR, DATA_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ed71a9-4ef9-4121-bab4-d5a7ce5bb66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc261876-3b78-4765-9842-d30fcfb5c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97d1ecb-c5fb-43b5-ae43-ffdcc999b0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[ 'datetime', 't', 'p', 'u', 'dd', 'ff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a9c16e-ca32-4884-ac50-5412a5198b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clmns_name = {\n",
    "    't':'temp',\n",
    "    'p': 'pressure',\n",
    "    'u':'humidity',\n",
    "    'dd':'wind_direction',\n",
    "    'ff': 'wind_speed'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40837260-96e0-4602-b464-367142a2aab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns=clmns_name, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be9a73e-e9ac-43bd-8bc9-a099b9cd091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6bf775-8225-4c06-9119-9740686b75e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3340cc98-2eed-4ca1-b1bf-62eb09539f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for exact duplicate rows\n",
    "print(\"Exact duplicate rows:\", df.duplicated().sum())\n",
    "\n",
    "# Check for duplicates based only on datetime (should be unique)\n",
    "print(\"Duplicate datetime entries:\", df.duplicated(subset=['datetime']).sum())\n",
    "\n",
    "# Show duplicated datetime entries (if any)\n",
    "duplicated_datetimes = df[df.duplicated(subset=['datetime'], keep=False)]\n",
    "print(\"\\nDuplicated datetime records (if any):\")\n",
    "print(duplicated_datetimes.sort_values('datetime').head(20))\n",
    "\n",
    "# Check for near-duplicates in datetime (e.g., same minute)\n",
    "df['datetime_rounded'] = pd.to_datetime(df['datetime']).dt.round('min')\n",
    "print(\"\\nRows with same rounded minute:\", df.duplicated(subset=['datetime_rounded']).sum())\n",
    "\n",
    "# Optional: inspect time gaps for irregular sampling\n",
    "df_sorted = df.sort_values('datetime').copy()\n",
    "df_sorted['time_diff'] = pd.to_datetime(df_sorted['datetime']).diff()\n",
    "print(\"\\nMost common time intervals:\")\n",
    "print(df_sorted['time_diff'].value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d4960b-8cda-4590-b703-1e170beb0fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify rows where humidity is still NaN after previous filling\n",
    "hum_nan_mask = df['humidity'].isna()\n",
    "\n",
    "# Show count of remaining NaNs\n",
    "print(\"Remaining humidity NaNs:\", hum_nan_mask.sum())\n",
    "\n",
    "# Show first few rows with humidity NaN\n",
    "print(\"\\nFirst 10 rows with humidity NaN:\")\n",
    "df[hum_nan_mask].head(10)\n",
    "\n",
    "# Check if remaining NaNs are at the start or end\n",
    "print(\"\\nNaNs in first 100 rows:\", hum_nan_mask.iloc[:100].sum())\n",
    "print(\"NaNs in last 100 rows:\", hum_nan_mask.iloc[-100:].sum())\n",
    "\n",
    "# Check time gaps around remaining NaNs (to see if part of long gaps)\n",
    "if hum_nan_mask.sum() > 0:\n",
    "    nan_df = df[hum_nan_mask].copy()\n",
    "    print(\"\\nSample datetime range of remaining NaNs:\")\n",
    "    print(\"Earliest:\", nan_df['datetime'].min())\n",
    "    print(\"Latest:\", nan_df['datetime'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ad7726-de60-4622-8cf6-122d4d3cbc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[hum_nan_mask].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ff1097-32ab-4c42-a484-822aa25c13af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure datetime is in correct datetime type and sort\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df = df.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "# Set datetime as index for time-based interpolation\n",
    "df = df.set_index('datetime')\n",
    "\n",
    "# Drop the helper column used earlier\n",
    "df = df.drop(columns=['datetime_rounded'], errors='ignore')\n",
    "\n",
    "# Time-based interpolation for remaining NaNs in all numeric columns\n",
    "df = df.interpolate(method='time')\n",
    "\n",
    "# Optional: forward/backward fill any remaining edge NaNs\n",
    "df = df.ffill().bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a28e02-333d-408c-b442-2cb784f87989",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9bb20f-6ec6-43ff-ac70-6bc781f6ef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Ensure datetime index is sorted (already done, but safe to confirm)\n",
    "assert df.index.is_monotonic_increasing, \"Datetime index is not sorted!\"\n",
    "\n",
    "# 2. Calculate time differences between consecutive rows\n",
    "time_diffs = df.index.to_series().diff()\n",
    "\n",
    "# 3. Show unique time intervals and their frequencies\n",
    "print(\"Most common time intervals:\")\n",
    "print(time_diffs.value_counts().head(10))\n",
    "\n",
    "# 4. Check for unexpected gaps (e.g., > 180 minutes)\n",
    "expected_freq = pd.Timedelta(minutes=10)  # adjust if your data has different freq\n",
    "large_gaps = time_diffs[time_diffs > pd.Timedelta(minutes=180)]\n",
    "print(f\"\\nNumber of large gaps (>180 min): {len(large_gaps)}\")\n",
    "if len(large_gaps) > 0:\n",
    "    print(\"First few large gaps:\")\n",
    "    print(large_gaps.head())\n",
    "\n",
    "# 5. Check for duplicate timestamps\n",
    "duplicated_times = df.index.duplicated()\n",
    "print(f\"\\nNumber of duplicate timestamps: {duplicated_times.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b7273a-98b3-41b5-aa30-42bab04d2b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39e9091-ce92-463d-89ac-775420faf187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Basic statistics\n",
    "print(\"Basic statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# 2. Lag-1 autocorrelation (expected to be high for meteorological variables)\n",
    "print(\"\\nLag-1 autocorrelation (values >0.8 typical for temp/pressure/humidity):\")\n",
    "autocorr = df[['temp', 'pressure', 'humidity', 'wind_speed']].apply(lambda x: x.autocorr(lag=1))\n",
    "print(autocorr)\n",
    "\n",
    "# 3. Histograms to identify anomalies or unrealistic distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "cols = ['temp', 'pressure', 'humidity', 'wind_speed', 'wind_direction']\n",
    "for i, col in enumerate(cols):\n",
    "    ax = axes[i // 3, i % 3]\n",
    "    df[col].hist(bins=50, ax=ax)\n",
    "    ax.set_title(f'{col} distribution')\n",
    "# Hide unused subplot\n",
    "axes[1, 2].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Check for excessive zero wind speed (some calm periods are normal; very high % may indicate issues)\n",
    "zero_wind_pct = (df['wind_speed'] == 0).mean() * 100\n",
    "print(f\"\\nPercentage of zero wind speed readings: {zero_wind_pct:.2f}%\")\n",
    "\n",
    "# 5. Wind direction sanity check: 0° and 360° are equivalent; excess of either may indicate default/fill values\n",
    "wind_dir_0 = (df['wind_direction'] == 0).sum()\n",
    "wind_dir_360 = (df['wind_direction'] == 360).sum()\n",
    "print(f\"Wind direction = 0°: {wind_dir_0} occurrences\")\n",
    "print(f\"Wind direction = 360°: {wind_dir_360} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e250e19-f9bc-4797-a176-d14cbb9b86fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Convert Meteorological Degrees to Radians for Math\n",
    "# Formula: (270 - degrees) aligns Meteo North (0) to Math North (90)\n",
    "wd_rad = np.deg2rad(270 - df['wind_direction'])\n",
    "\n",
    "# 2. Calculate U and V\n",
    "# U = East-West component, V = North-South component\n",
    "df['u'] = df['wind_speed'] * np.cos(wd_rad)\n",
    "df['v'] = df['wind_speed'] * np.sin(wd_rad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5925f74e-bf28-4216-9cc7-3b132970c1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"tab10\")\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Wind vectors (u vs v) colored by time index\n",
    "sc = axes[0, 0].scatter(df['u'], df['v'], \n",
    "                        c=range(len(df)), cmap='plasma', alpha=0.6, s=10)\n",
    "axes[0, 0].set_xlabel('u (zonal, eastward)')\n",
    "axes[0, 0].set_ylabel('v (meridional, northward)')\n",
    "axes[0, 0].set_title('Wind Vector Distribution (colored by time)')\n",
    "plt.colorbar(sc, ax=axes[0, 0], label='Time (hour index)')\n",
    "\n",
    "# 2. Temperature and humidity over last 30 days\n",
    "last_30d = df.last('30D')\n",
    "ax2 = axes[0, 1].twinx()\n",
    "ln1 = axes[0, 1].plot(last_30d.index, last_30d['temp'], \n",
    "                      color='tab:red', label='Temperature', alpha=0.8)\n",
    "ln2 = ax2.plot(last_30d.index, last_30d['humidity'], \n",
    "               color='tab:blue', label='Humidity', alpha=0.8)\n",
    "axes[0, 1].set_xlabel('Date')\n",
    "axes[0, 1].set_ylabel('Temperature (°C)', color='tab:red')\n",
    "ax2.set_ylabel('Humidity (%)', color='tab:blue')\n",
    "axes[0, 1].set_title('Last 30 Days: Temperature & Humidity')\n",
    "lines = ln1 + ln2\n",
    "labels = [l.get_label() for l in lines]\n",
    "axes[0, 1].legend(lines, labels, loc='upper left')\n",
    "\n",
    "# 3. Pressure trend with 24-hour rolling mean\n",
    "df['pressure_24h'] = df['pressure'].rolling(window=24, min_periods=1).mean()\n",
    "axes[1, 0].plot(df.index, df['pressure'], alpha=0.3, color='gray', label='Hourly')\n",
    "axes[1, 0].plot(df.index, df['pressure_24h'], color='tab:green', label='24h avg')\n",
    "axes[1, 0].set_xlabel('Date')\n",
    "axes[1, 0].set_ylabel('Pressure (hPa)')\n",
    "axes[1, 0].set_title('Pressure Trend (2465m)')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. Wind speed distribution computed from u and v components\n",
    "wind_speed_from_uv = np.sqrt(df['u']**2 + df['v']**2)\n",
    "axes[1, 1].hist(wind_speed_from_uv, bins=50, color='tab:orange', alpha=0.7, edgecolor='k')\n",
    "axes[1, 1].set_xlabel('Wind Speed (m/s) — derived from u/v')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Wind Speed Distribution (from u/v components)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1a9033-34f2-4b9c-ad26-550f0e5d6521",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26c83c5-2be7-40e5-b20f-02078beeadff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ===========================\n",
    "# 6-d. EXPORT HOURLY CLEAN CSV\n",
    "# ===========================\n",
    "out_dir = '../../data/cleaned/'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "output_path = os.path.join(out_dir, 'bandarAbas_multi_var_wind_3hourly_cleaned.csv')\n",
    "df.to_csv(output_path)\n",
    "\n",
    "print(\"✅ Multi-variate 3hourly clean file saved to:\", hourly_path)\n",
    "print(\"Shape written:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d119fab1-71a0-48a9-b643-55377d4a2847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4e70d8-1494-4572-832e-dc8761b6473e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
